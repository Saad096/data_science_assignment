{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Task2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def load_and_preprocess_dataset(dataset_path, train_size, val_size, test_size, batch_size):\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Handle missing values\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # One-hot encode categorical variables\n",
        "    encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "    sex_encoded = encoder.fit_transform(df[['Sex']])\n",
        "    sex_encoded = pd.DataFrame(sex_encoded, columns=['Sex_male'])\n",
        "\n",
        "    # Normalize numerical features\n",
        "    scaler = StandardScaler()\n",
        "    numerical_features = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked', 'Survived', 'Sex'])\n",
        "    numerical_features = pd.DataFrame(scaler.fit_transform(numerical_features), columns=numerical_features.columns)\n",
        "\n",
        "    # Combine encoded features with numerical features\n",
        "    X = pd.concat([numerical_features, sex_encoded], axis=1)\n",
        "    y = df['Survived']\n",
        "\n",
        "    # Train-validation-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size/(train_size+val_size), random_state=42)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train.values).float(), torch.tensor(y_train.values).long())\n",
        "    val_dataset = TensorDataset(torch.tensor(X_val.values).float(), torch.tensor(y_val.values).long())\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test.values).float(), torch.tensor(y_test.values).long())\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Step 2: Model Architecture\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Step 3: Training Process\n",
        "def train(model, train_loader, val_loader, lr, n_epochs):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item() * inputs.size(0)\n",
        "        epoch_train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            epoch_val_loss = 0.0\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "                epoch_val_loss += loss.item() * inputs.size(0)\n",
        "            epoch_val_loss /= len(val_loader.dataset)\n",
        "            val_losses.append(epoch_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "# Step 4: Testing\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.round(outputs)\n",
        "            y_true.extend(labels.numpy())\n",
        "            y_pred.extend(predicted.numpy().flatten().astype(int))\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    return accuracy, f1, cm\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Step 1: Load and preprocess dataset\n",
        "    train_loader, val_loader, test_loader = load_and_preprocess_dataset('/content/train.csv', train_size=0.6, val_size=0.2, test_size=0.2, batch_size=32)\n",
        "\n",
        "    # Step 2: Initialize Network\n",
        "    model = LogisticRegression(input_size=train_loader.dataset.tensors[0].shape[1])\n",
        "\n",
        "    # Step 3: Training\n",
        "    model, train_losses, val_losses = train(model, train_loader, val_loader, lr=0.01, n_epochs=100)\n",
        "\n",
        "    # Step 4: Testing\n",
        "    accuracy, f1, cm = test(model, test_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsAtxdDtYEaE",
        "outputId": "7eecfc9e-e060-4f7d-e20d-3f74551e0110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6865, Val Loss: 0.6589\n",
            "Epoch 2/100, Train Loss: 0.6698, Val Loss: 0.6451\n",
            "Epoch 3/100, Train Loss: 0.6545, Val Loss: 0.6328\n",
            "Epoch 4/100, Train Loss: 0.6411, Val Loss: 0.6219\n",
            "Epoch 5/100, Train Loss: 0.6290, Val Loss: 0.6120\n",
            "Epoch 6/100, Train Loss: 0.6181, Val Loss: 0.6032\n",
            "Epoch 7/100, Train Loss: 0.6083, Val Loss: 0.5954\n",
            "Epoch 8/100, Train Loss: 0.5997, Val Loss: 0.5884\n",
            "Epoch 9/100, Train Loss: 0.5919, Val Loss: 0.5821\n",
            "Epoch 10/100, Train Loss: 0.5848, Val Loss: 0.5764\n",
            "Epoch 11/100, Train Loss: 0.5785, Val Loss: 0.5712\n",
            "Epoch 12/100, Train Loss: 0.5728, Val Loss: 0.5666\n",
            "Epoch 13/100, Train Loss: 0.5675, Val Loss: 0.5624\n",
            "Epoch 14/100, Train Loss: 0.5629, Val Loss: 0.5586\n",
            "Epoch 15/100, Train Loss: 0.5587, Val Loss: 0.5552\n",
            "Epoch 16/100, Train Loss: 0.5548, Val Loss: 0.5520\n",
            "Epoch 17/100, Train Loss: 0.5512, Val Loss: 0.5491\n",
            "Epoch 18/100, Train Loss: 0.5481, Val Loss: 0.5465\n",
            "Epoch 19/100, Train Loss: 0.5451, Val Loss: 0.5440\n",
            "Epoch 20/100, Train Loss: 0.5423, Val Loss: 0.5417\n",
            "Epoch 21/100, Train Loss: 0.5397, Val Loss: 0.5395\n",
            "Epoch 22/100, Train Loss: 0.5373, Val Loss: 0.5376\n",
            "Epoch 23/100, Train Loss: 0.5352, Val Loss: 0.5357\n",
            "Epoch 24/100, Train Loss: 0.5331, Val Loss: 0.5340\n",
            "Epoch 25/100, Train Loss: 0.5313, Val Loss: 0.5324\n",
            "Epoch 26/100, Train Loss: 0.5295, Val Loss: 0.5309\n",
            "Epoch 27/100, Train Loss: 0.5278, Val Loss: 0.5295\n",
            "Epoch 28/100, Train Loss: 0.5262, Val Loss: 0.5281\n",
            "Epoch 29/100, Train Loss: 0.5248, Val Loss: 0.5269\n",
            "Epoch 30/100, Train Loss: 0.5234, Val Loss: 0.5256\n",
            "Epoch 31/100, Train Loss: 0.5221, Val Loss: 0.5244\n",
            "Epoch 32/100, Train Loss: 0.5208, Val Loss: 0.5233\n",
            "Epoch 33/100, Train Loss: 0.5196, Val Loss: 0.5222\n",
            "Epoch 34/100, Train Loss: 0.5185, Val Loss: 0.5211\n",
            "Epoch 35/100, Train Loss: 0.5173, Val Loss: 0.5201\n",
            "Epoch 36/100, Train Loss: 0.5163, Val Loss: 0.5191\n",
            "Epoch 37/100, Train Loss: 0.5153, Val Loss: 0.5182\n",
            "Epoch 38/100, Train Loss: 0.5143, Val Loss: 0.5173\n",
            "Epoch 39/100, Train Loss: 0.5134, Val Loss: 0.5164\n",
            "Epoch 40/100, Train Loss: 0.5125, Val Loss: 0.5155\n",
            "Epoch 41/100, Train Loss: 0.5117, Val Loss: 0.5147\n",
            "Epoch 42/100, Train Loss: 0.5108, Val Loss: 0.5139\n",
            "Epoch 43/100, Train Loss: 0.5100, Val Loss: 0.5131\n",
            "Epoch 44/100, Train Loss: 0.5092, Val Loss: 0.5124\n",
            "Epoch 45/100, Train Loss: 0.5084, Val Loss: 0.5116\n",
            "Epoch 46/100, Train Loss: 0.5077, Val Loss: 0.5109\n",
            "Epoch 47/100, Train Loss: 0.5070, Val Loss: 0.5102\n",
            "Epoch 48/100, Train Loss: 0.5062, Val Loss: 0.5095\n",
            "Epoch 49/100, Train Loss: 0.5056, Val Loss: 0.5089\n",
            "Epoch 50/100, Train Loss: 0.5049, Val Loss: 0.5082\n",
            "Epoch 51/100, Train Loss: 0.5043, Val Loss: 0.5076\n",
            "Epoch 52/100, Train Loss: 0.5037, Val Loss: 0.5070\n",
            "Epoch 53/100, Train Loss: 0.5031, Val Loss: 0.5063\n",
            "Epoch 54/100, Train Loss: 0.5025, Val Loss: 0.5057\n",
            "Epoch 55/100, Train Loss: 0.5020, Val Loss: 0.5051\n",
            "Epoch 56/100, Train Loss: 0.5014, Val Loss: 0.5045\n",
            "Epoch 57/100, Train Loss: 0.5008, Val Loss: 0.5040\n",
            "Epoch 58/100, Train Loss: 0.5002, Val Loss: 0.5034\n",
            "Epoch 59/100, Train Loss: 0.4997, Val Loss: 0.5029\n",
            "Epoch 60/100, Train Loss: 0.4992, Val Loss: 0.5023\n",
            "Epoch 61/100, Train Loss: 0.4987, Val Loss: 0.5018\n",
            "Epoch 62/100, Train Loss: 0.4982, Val Loss: 0.5012\n",
            "Epoch 63/100, Train Loss: 0.4977, Val Loss: 0.5007\n",
            "Epoch 64/100, Train Loss: 0.4972, Val Loss: 0.5002\n",
            "Epoch 65/100, Train Loss: 0.4967, Val Loss: 0.4997\n",
            "Epoch 66/100, Train Loss: 0.4963, Val Loss: 0.4992\n",
            "Epoch 67/100, Train Loss: 0.4958, Val Loss: 0.4987\n",
            "Epoch 68/100, Train Loss: 0.4954, Val Loss: 0.4982\n",
            "Epoch 69/100, Train Loss: 0.4949, Val Loss: 0.4978\n",
            "Epoch 70/100, Train Loss: 0.4945, Val Loss: 0.4973\n",
            "Epoch 71/100, Train Loss: 0.4941, Val Loss: 0.4968\n",
            "Epoch 72/100, Train Loss: 0.4936, Val Loss: 0.4964\n",
            "Epoch 73/100, Train Loss: 0.4932, Val Loss: 0.4959\n",
            "Epoch 74/100, Train Loss: 0.4928, Val Loss: 0.4954\n",
            "Epoch 75/100, Train Loss: 0.4924, Val Loss: 0.4950\n",
            "Epoch 76/100, Train Loss: 0.4920, Val Loss: 0.4946\n",
            "Epoch 77/100, Train Loss: 0.4916, Val Loss: 0.4941\n",
            "Epoch 78/100, Train Loss: 0.4912, Val Loss: 0.4937\n",
            "Epoch 79/100, Train Loss: 0.4909, Val Loss: 0.4933\n",
            "Epoch 80/100, Train Loss: 0.4905, Val Loss: 0.4929\n",
            "Epoch 81/100, Train Loss: 0.4901, Val Loss: 0.4924\n",
            "Epoch 82/100, Train Loss: 0.4897, Val Loss: 0.4920\n",
            "Epoch 83/100, Train Loss: 0.4894, Val Loss: 0.4916\n",
            "Epoch 84/100, Train Loss: 0.4890, Val Loss: 0.4912\n",
            "Epoch 85/100, Train Loss: 0.4887, Val Loss: 0.4908\n",
            "Epoch 86/100, Train Loss: 0.4883, Val Loss: 0.4905\n",
            "Epoch 87/100, Train Loss: 0.4880, Val Loss: 0.4901\n",
            "Epoch 88/100, Train Loss: 0.4877, Val Loss: 0.4897\n",
            "Epoch 89/100, Train Loss: 0.4873, Val Loss: 0.4893\n",
            "Epoch 90/100, Train Loss: 0.4870, Val Loss: 0.4889\n",
            "Epoch 91/100, Train Loss: 0.4866, Val Loss: 0.4885\n",
            "Epoch 92/100, Train Loss: 0.4864, Val Loss: 0.4882\n",
            "Epoch 93/100, Train Loss: 0.4860, Val Loss: 0.4878\n",
            "Epoch 94/100, Train Loss: 0.4857, Val Loss: 0.4875\n",
            "Epoch 95/100, Train Loss: 0.4855, Val Loss: 0.4872\n",
            "Epoch 96/100, Train Loss: 0.4851, Val Loss: 0.4868\n",
            "Epoch 97/100, Train Loss: 0.4849, Val Loss: 0.4865\n",
            "Epoch 98/100, Train Loss: 0.4845, Val Loss: 0.4861\n",
            "Epoch 99/100, Train Loss: 0.4843, Val Loss: 0.4858\n",
            "Epoch 100/100, Train Loss: 0.4840, Val Loss: 0.4855\n",
            "Accuracy: 0.776536312849162, F1 Score: 0.6666666666666666\n",
            "Confusion Matrix:\n",
            "[[99  6]\n",
            " [34 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# task1\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pickle\n",
        "\n",
        "# Function to split the data into train, validation, and test sets\n",
        "def data_split(X, Y):\n",
        "    n = X.shape[0]\n",
        "    train_size = int(0.7 * n)\n",
        "    val_size = int(0.2 * n)\n",
        "    train_X, train_Y = X[:train_size], Y[:train_size]\n",
        "    val_X, val_Y = X[train_size:train_size+val_size], Y[train_size:train_size+val_size]\n",
        "    test_X, test_Y = X[train_size+val_size:], Y[train_size+val_size:]\n",
        "    return train_X, train_Y, val_X, val_Y, test_X, test_Y\n",
        "\n",
        "# Function to normalize the data\n",
        "def normalize(X):\n",
        "    mean = np.mean(X, axis=0)\n",
        "    std = np.std(X, axis=0)\n",
        "    normalized_X = (X - mean) / std\n",
        "    return normalized_X, mean, std\n",
        "\n",
        "# Function to initialize network\n",
        "def linear_regression_network(input_size):\n",
        "    theta = np.random.randn(input_size, 1)\n",
        "    return theta\n",
        "\n",
        "# Function for feed forward\n",
        "def feed_forward(X, theta):\n",
        "    y_hat = np.dot(X, theta)\n",
        "    return y_hat\n",
        "\n",
        "# Function to compute loss\n",
        "def l2_loss(y_true, y_pred):\n",
        "    loss = np.mean((y_pred - y_true) ** 2)\n",
        "    return loss\n",
        "\n",
        "# Function to compute gradient\n",
        "def compute_gradient(X, y_true, y_pred):\n",
        "    gradient = np.dot(X.T, (y_pred - y_true)) / len(X)\n",
        "    return gradient\n",
        "\n",
        "# Function for optimization (SGD)\n",
        "def optimization(lr, gradient, theta):\n",
        "    theta -= lr * gradient\n",
        "    return theta\n",
        "\n",
        "# Function to train the model\n",
        "def train(net, train_X, train_Y, val_X, val_Y, batch_size, n_epochs, lr):\n",
        "    loss_epoch_tr = []\n",
        "    loss_epoch_val = []\n",
        "    for epoch in range(n_epochs):\n",
        "        for i in range(0, len(train_X), batch_size):\n",
        "            X_batch = train_X[i:i+batch_size]\n",
        "            y_batch = train_Y[i:i+batch_size]\n",
        "            y_hat = feed_forward(X_batch, net)\n",
        "            loss = l2_loss(y_batch, y_hat)\n",
        "            gradient = compute_gradient(X_batch, y_batch, y_hat)\n",
        "            net = optimization(lr, gradient, net)\n",
        "        loss_epoch_tr.append(loss)\n",
        "        val_y_hat = feed_forward(val_X, net)\n",
        "        val_loss = l2_loss(val_Y, val_y_hat)\n",
        "        loss_epoch_val.append(val_loss)\n",
        "        print(f'Epoch {epoch+1}/{n_epochs}, Training Loss: {loss}, Validation Loss: {val_loss}')\n",
        "    return net, loss_epoch_tr, loss_epoch_val\n",
        "\n",
        "# Function to test the model\n",
        "def test_function(model, test_X, test_Y):\n",
        "    with open(model, 'rb') as f:\n",
        "        trained_model = pickle.load(f)\n",
        "    y_hat = feed_forward(test_X, trained_model)\n",
        "    test_loss = l2_loss(test_Y, y_hat)\n",
        "    print(f'Test Loss: {test_loss}')\n",
        "    return test_loss\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load dataset\n",
        "    dataset = fetch_california_housing()\n",
        "    X = dataset.data\n",
        "    Y = dataset.target[:, np.newaxis]\n",
        "\n",
        "    # Split data\n",
        "    train_X, train_Y, val_X, val_Y, test_X, test_Y = data_split(X, Y)\n",
        "\n",
        "    # Normalize data\n",
        "    train_X, mean, std = normalize(train_X)\n",
        "    val_X = (val_X - mean) / std\n",
        "    test_X = (test_X - mean) / std\n",
        "\n",
        "    # Initialize network\n",
        "    net = linear_regression_network(X.shape[1])\n",
        "\n",
        "    # Train the model\n",
        "    batch_size = 32\n",
        "    n_epochs = 100\n",
        "    lr = 0.01\n",
        "    trained_model, loss_epoch_tr, loss_epoch_val = train(net, train_X, train_Y, val_X, val_Y, batch_size, n_epochs, lr)\n",
        "\n",
        "    # Save trained model\n",
        "    with open('model.pkl', 'wb') as f:\n",
        "        pickle.dump(trained_model, f)\n",
        "\n",
        "    # Test the model\n",
        "    test_function('model.pkl', test_X, test_Y)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ukma5bWYcOG",
        "outputId": "58fb5919-219d-425a-bf03-952331433e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Training Loss: 3.8035481100895496, Validation Loss: 9.561462200661463\n",
            "Epoch 2/100, Training Loss: 3.502288133637066, Validation Loss: 9.254675189607683\n",
            "Epoch 3/100, Training Loss: 3.820291178034654, Validation Loss: 9.230700057299183\n",
            "Epoch 4/100, Training Loss: 3.455070581197364, Validation Loss: 9.078773220867811\n",
            "Epoch 5/100, Training Loss: 3.7709056634994442, Validation Loss: 9.101452901952376\n",
            "Epoch 6/100, Training Loss: 3.42849988836921, Validation Loss: 8.980421945428384\n",
            "Epoch 7/100, Training Loss: 3.7420963365307944, Validation Loss: 9.024218500266347\n",
            "Epoch 8/100, Training Loss: 3.416010235766616, Validation Loss: 8.920644923161374\n",
            "Epoch 9/100, Training Loss: 3.724402618729343, Validation Loss: 8.975316720778393\n",
            "Epoch 10/100, Training Loss: 3.410959481752047, Validation Loss: 8.882368018084586\n",
            "Epoch 11/100, Training Loss: 3.712701931369354, Validation Loss: 8.9424311315647\n",
            "Epoch 12/100, Training Loss: 3.4098680095217806, Validation Loss: 8.856432387894772\n",
            "Epoch 13/100, Training Loss: 3.704303245984071, Validation Loss: 8.918883888012171\n",
            "Epoch 14/100, Training Loss: 3.410895276672527, Validation Loss: 8.837805905890319\n",
            "Epoch 15/100, Training Loss: 3.69777418793972, Validation Loss: 8.900973782583652\n",
            "Epoch 16/100, Training Loss: 3.413055460550377, Validation Loss: 8.823662125175078\n",
            "Epoch 17/100, Training Loss: 3.692341630812195, Validation Loss: 8.886602764451554\n",
            "Epoch 18/100, Training Loss: 3.4158137726190114, Validation Loss: 8.812376404979213\n",
            "Epoch 19/100, Training Loss: 3.6875816598652125, Validation Loss: 8.874552557256912\n",
            "Epoch 20/100, Training Loss: 3.4188761639487457, Validation Loss: 8.802991575396073\n",
            "Epoch 21/100, Training Loss: 3.6832575133197736, Validation Loss: 8.864097108293834\n",
            "Epoch 22/100, Training Loss: 3.4220786552870597, Validation Loss: 8.79492867495368\n",
            "Epoch 23/100, Training Loss: 3.679234048933323, Validation Loss: 8.854791477623378\n",
            "Epoch 24/100, Training Loss: 3.4253285115258034, Validation Loss: 8.787827642552534\n",
            "Epoch 25/100, Training Loss: 3.6754321364979265, Validation Loss: 8.846354814385139\n",
            "Epoch 26/100, Training Loss: 3.4285726299869514, Validation Loss: 8.781457958327241\n",
            "Epoch 27/100, Training Loss: 3.671804055475407, Validation Loss: 8.838604255001322\n",
            "Epoch 28/100, Training Loss: 3.431780338258519, Validation Loss: 8.77566752517752\n",
            "Epoch 29/100, Training Loss: 3.668320043228643, Validation Loss: 8.831416822895426\n",
            "Epoch 30/100, Training Loss: 3.434933896194216, Validation Loss: 8.770352812577793\n",
            "Epoch 31/100, Training Loss: 3.6649608242811693, Validation Loss: 8.824706997324236\n",
            "Epoch 32/100, Training Loss: 3.4380231639582903, Validation Loss: 8.765441042918425\n",
            "Epoch 33/100, Training Loss: 3.6617133872260945, Validation Loss: 8.818413219297144\n",
            "Epoch 34/100, Training Loss: 3.4410425531247295, Validation Loss: 8.760879335327441\n",
            "Epoch 35/100, Training Loss: 3.658568550596238, Validation Loss: 8.812489599805954\n",
            "Epoch 36/100, Training Loss: 3.4439892484538683, Validation Loss: 8.756627953375094\n",
            "Epoch 37/100, Training Loss: 3.655519530598478, Validation Loss: 8.806900720773541\n",
            "Epoch 38/100, Training Loss: 3.446862149400084, Validation Loss: 8.752656024621052\n",
            "Epoch 39/100, Training Loss: 3.6525610803738586, Validation Loss: 8.801618313550602\n",
            "Epoch 40/100, Training Loss: 3.4496612271976788, Validation Loss: 8.748938779548842\n",
            "Epoch 41/100, Training Loss: 3.649688961854795, Validation Loss: 8.796619100398896\n",
            "Epoch 42/100, Training Loss: 3.4523871268257222, Validation Loss: 8.745455742369268\n",
            "Epoch 43/100, Training Loss: 3.646899615224797, Validation Loss: 8.791883369876073\n",
            "Epoch 44/100, Training Loss: 3.4550409162710074, Validation Loss: 8.742189528516677\n",
            "Epoch 45/100, Training Loss: 3.6441899482291698, Validation Loss: 8.787394023136695\n",
            "Epoch 46/100, Training Loss: 3.4576239261890933, Validation Loss: 8.739125034748485\n",
            "Epoch 47/100, Training Loss: 3.641557199633526, Validation Loss: 8.783135926839979\n",
            "Epoch 48/100, Training Loss: 3.460137646095374, Validation Loss: 8.736248886659736\n",
            "Epoch 49/100, Training Loss: 3.6389988494029266, Validation Loss: 8.779095468205176\n",
            "Epoch 50/100, Training Loss: 3.4625836565100703, Validation Loss: 8.733549056879154\n",
            "Epoch 51/100, Training Loss: 3.636512558804997, Validation Loss: 8.775260244785041\n",
            "Epoch 52/100, Training Loss: 3.46496358430957, Validation Loss: 8.73101459753353\n",
            "Epoch 53/100, Training Loss: 3.634096129953172, Validation Loss: 8.771618844861488\n",
            "Epoch 54/100, Training Loss: 3.4672790732429273, Validation Loss: 8.728635449863104\n",
            "Epoch 55/100, Training Loss: 3.6317474781324703, Validation Loss: 8.768160689312012\n",
            "Epoch 56/100, Training Loss: 3.4695317644596404, Validation Loss: 8.726402306332426\n",
            "Epoch 57/100, Training Loss: 3.6294646126160988, Validation Loss: 8.764875915501424\n",
            "Epoch 58/100, Training Loss: 3.4717232836996668, Validation Loss: 8.72430650873027\n",
            "Epoch 59/100, Training Loss: 3.627245623170273, Validation Loss: 8.761755290132475\n",
            "Epoch 60/100, Training Loss: 3.4738552329443615, Validation Loss: 8.722339971136089\n",
            "Epoch 61/100, Training Loss: 3.6250886703977656, Validation Loss: 8.758790142221166\n",
            "Epoch 62/100, Training Loss: 3.475929185067891, Validation Loss: 8.720495120217706\n",
            "Epoch 63/100, Training Loss: 3.6229919786889138, Validation Loss: 8.755972310193004\n",
            "Epoch 64/100, Training Loss: 3.4779466805127597, Validation Loss: 8.718764847730881\n",
            "Epoch 65/100, Training Loss: 3.6209538309546048, Validation Loss: 8.753294099000822\n",
            "Epoch 66/100, Training Loss: 3.4799092253330004, Validation Loss: 8.717142471713924\n",
            "Epoch 67/100, Training Loss: 3.6189725645847566, Validation Loss: 8.750748244452831\n",
            "Epoch 68/100, Training Loss: 3.481818290161571, Validation Loss: 8.715621703969383\n",
            "Epoch 69/100, Training Loss: 3.617046568255439, Validation Loss: 8.748327882813701\n",
            "Epoch 70/100, Training Loss: 3.483675309801364, Validation Loss: 8.71419662217157\n",
            "Epoch 71/100, Training Loss: 3.615174279328398, Validation Loss: 8.746026524337413\n",
            "Epoch 72/100, Training Loss: 3.4854816832355113, Validation Loss: 8.712861645447832\n",
            "Epoch 73/100, Training Loss: 3.613354181668322, Validation Loss: 8.743838029797077\n",
            "Epoch 74/100, Training Loss: 3.487238773917933, Validation Loss: 8.711611512629196\n",
            "Epoch 75/100, Training Loss: 3.611584803758238, Validation Loss: 8.741756589355944\n",
            "Epoch 76/100, Training Loss: 3.4889479102492533, Validation Loss: 8.710441262604432\n",
            "Epoch 77/100, Training Loss: 3.6098647170311464, Validation Loss: 8.739776703314744\n",
            "Epoch 78/100, Training Loss: 3.490610386173392, Validation Loss: 8.709346216375137\n",
            "Epoch 79/100, Training Loss: 3.6081925343614185, Validation Loss: 8.737893164402433\n",
            "Epoch 80/100, Training Loss: 3.4922274618506988, Validation Loss: 8.708321960522243\n",
            "Epoch 81/100, Training Loss: 3.606566908676983, Validation Loss: 8.736101041368187\n",
            "Epoch 82/100, Training Loss: 3.4938003643774835, Validation Loss: 8.70736433187208\n",
            "Epoch 83/100, Training Loss: 3.604986531665253, Validation Loss: 8.73439566369551\n",
            "Epoch 84/100, Training Loss: 3.4953302885314455, Validation Loss: 8.706469403204261\n",
            "Epoch 85/100, Training Loss: 3.6034501325539177, Validation Loss: 8.73277260730331\n",
            "Epoch 86/100, Training Loss: 3.4968183975290104, Validation Loss: 8.70563346988131\n",
            "Epoch 87/100, Training Loss: 3.6019564769532963, Validation Loss: 8.731227681129383\n",
            "Epoch 88/100, Training Loss: 3.4982658237851414, Validation Loss: 8.704853037306362\n",
            "Epoch 89/100, Training Loss: 3.6005043657508127, Validation Loss: 8.72975691451361\n",
            "Epoch 90/100, Training Loss: 3.4996736696692143, Validation Loss: 8.704124809134061\n",
            "Epoch 91/100, Training Loss: 3.599092634050797, Validation Loss: 8.728356545313517\n",
            "Epoch 92/100, Training Loss: 3.5010430082526685, Validation Loss: 8.703445676173171\n",
            "Epoch 93/100, Training Loss: 3.597720150154638, Validation Loss: 8.7270230086961\n",
            "Epoch 94/100, Training Loss: 3.502374884045657, Validation Loss: 8.702812705929139\n",
            "Epoch 95/100, Training Loss: 3.596385814577539, Validation Loss: 8.725752926558142\n",
            "Epoch 96/100, Training Loss: 3.5036703137207525, Validation Loss: 8.702223132742185\n",
            "Epoch 97/100, Training Loss: 3.5950885590990973, Validation Loss: 8.724543097533111\n",
            "Epoch 98/100, Training Loss: 3.5049302868226513, Validation Loss: 8.701674348481694\n",
            "Epoch 99/100, Training Loss: 3.5938273458453853, Validation Loss: 8.723390487547656\n",
            "Epoch 100/100, Training Loss: 3.506155766463122, Validation Loss: 8.701163893762079\n",
            "Test Loss: 6.702966752364681\n"
          ]
        }
      ]
    }
  ]
}